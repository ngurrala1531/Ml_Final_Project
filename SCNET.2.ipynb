{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtkpYxbE88QJ",
        "outputId": "385e27d4-b081-4b7c-8715-85d882ed7c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bhg4_CLBWU3",
        "outputId": "5f6cbf94-c46f-4c6c-8817-1f1a776cb0e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'SCNet' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "dataset_path = '/content/drive/MyDrive/dataset'\n",
        "!git clone https://github.com/backseason/SCNet.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lMz7spnE5CZ",
        "outputId": "4fb402d0-40bc-434f-f420-9940d5fd2b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/SCNet\n"
          ]
        }
      ],
      "source": [
        "%cd SCNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hjMCyHNFGv6"
      },
      "outputs": [],
      "source": [
        "from scnet import scnet50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKaS6Nk2BaPx",
        "outputId": "8822192e-95d7-43df-9785-253c5dc1cd60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/2\n",
            "  Batch 10/97, Loss: 1.1847\n",
            "  Batch 20/97, Loss: 0.4528\n",
            "  Batch 30/97, Loss: 0.3626\n",
            "  Batch 40/97, Loss: 0.3535\n",
            "  Batch 50/97, Loss: 0.3078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Batch 60/97, Loss: 0.4472\n",
            "  Batch 70/97, Loss: 0.3084\n",
            "  Batch 80/97, Loss: 0.3951\n",
            "  Batch 90/97, Loss: 0.4027\n",
            "Training Loss: 0.5382\n",
            "  Validation Batch 10/25\n",
            "  Validation Batch 20/25\n",
            "Validation Loss: 0.4294, Accuracy: 78.65%\n",
            "Epoch 2/2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import 'scnet50' from Google Drive\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "\n",
        "from scnet import scnet50\n",
        "\n",
        "# Continue with the rest of your code...\n",
        "\n",
        "# Define the model\n",
        "model = scnet50(pretrained=False)  # Assuming you're not using a pre-trained model\n",
        "# Modify the last layer for binary classification (with_mask, without_mask)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "\n",
        "# Set up data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "dataset_path = '/content/drive/MyDrive/dataset'  # Update the path\n",
        "train_dataset = ImageFolder(dataset_path, transform=transform)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 0:  # Print every 10 batches\n",
        "            print(f'  Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    average_train_loss = running_loss / len(train_loader)\n",
        "    print(f'Training Loss: {average_train_loss:.4f}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (inputs, labels) in enumerate(val_loader, 1):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if i % 10 == 0:  # Print every 10 batches\n",
        "                print(f'  Validation Batch {i}/{len(val_loader)}')\n",
        "\n",
        "        average_val_loss = val_loss / len(val_loader)\n",
        "        accuracy = correct / total\n",
        "        print(f'Validation Loss: {average_val_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/path/to/mask_detection_model.pth')\n",
        "print('Model saved.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGctiDwHBxDZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}